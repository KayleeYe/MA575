---
title: "report4"
author: "Team1"
date: "2024-04-07"
output: html_document
---

```{r}
library(car)
library(caret)
library(ggplot2)
library(dplyr)
library(GGally)
library(readxl)
library(MLmetrics)
```

```{r}
#load the dataset
mydata <- read.csv('./data/day.csv')

```

```{r}
#convert 'dteday' column to Date format
mydata$dteday <- as.Date(mydata$dteday)

#season
mydata$season <- cut(mydata$season,
                     breaks = c(0.5, 1.5, 2.5, 3.5, 4.5),
                     labels = c("Winter", "Spring", "Summer", "Fall"))
mydata$season <- factor(mydata$season, levels = c("Winter", "Spring", "Summer", "Fall"))

#workingday
mydata$workingday <- ifelse(mydata$workingday == 0, "Not_Workingday", "Workingday")
mydata$workingday <- factor(mydata$workingday, levels = c("Not_Workingday", "Workingday"))

#weather
mydata$weathersit <- cut(mydata$weathersit,
                         breaks = c(0.5, 1.5, 2.5, 3.5, 4.5),
                         labels = c("Weather_1", "Weather_2", "Weather_3", "Weather_4"))
mydata$weathersit <- factor(mydata$weathersit, levels = c("Weather_1", "Weather_2", "Weather_3", "Weather_4"))

head(mydata)
```

# Data Splitting (Training & Validation): We split the data into training (year = 2011) and validation sets (year = 2012)

The year 2012 dataset will be used for prediction.

```{r}
# Set a seed for reproducibility
# Setting a seed is crucial for reproducibility and consistency of results.
# It ensures that when you run the same code multiple times, you'll ...
# ... obtain the same results.
set.seed(20231103)
# Filter dataset for year 2011 (0) and year 2012 (1)
year2011 <- mydata[mydata$yr == 0, ]
year2012 <- mydata[mydata$yr == 1, ]

# Add 'Type' column and assign values
year2011$Type <- "Training"
year2012$Type <- "Validation"

# Combine training and validation data
mydata <- rbind(year2011, year2012)
```

```{r}
#creating training data our of mydata (training -> data from 2011)
training_data <- subset(mydata, Type == "Training")
summary(training_data)
```

```{r}
#creating training data our of mydata (validatioin -> data from 2012)
validation_data <- subset(mydata, Type == "Validation")
summary(validation_data)
```

We choose 3 questions of interest:

1.  **How does working day affect the number of registered users?** where Y is the numnber of registered users
2.  **How does season affect the number of casual users?** where Y is the sqrt(casual users)
3.  **Is weather correlated to the number of bike rentals?\*** where Y is the count of bikes rented

# Datasets Comparison: Training vs. Validation

### Boxplots for proposed covariates y

```{r}

#registered
ggplot(mydata, aes(x = Type, y = registered, color = Type)) +
  geom_boxplot(position = position_dodge(width = 0.8)) +
  labs(x = "Type", y = "# Registered Users") +
  scale_fill_manual(name = "Dataset Type", values = c("Training" = "blue", "Validation" = "red")) +
  ggtitle("# Registered Users by Type (Training vs. Validation)") +
  theme_bw()

#sqrt(casual)
ggplot(mydata, aes(x = Type, y = sqrt(casual), color = Type)) +
  geom_boxplot(position = position_dodge(width = 0.8)) +
  labs(x = "Type", y = "Square Root # Casual Users") +
  scale_fill_manual(name = "Dataset Type", values = c("Training" = "blue", "Validation" = "red")) +
  ggtitle("Square Root # Casual Users by Type (Training vs. Validation)") +
  theme_bw()

#cnt
ggplot(mydata, aes(x = Type, y = cnt, color = Type)) +
  geom_boxplot(position = position_dodge(width = 0.8)) +
  labs(x = "Type", y = "# Total Users") +
  scale_fill_manual(name = "Dataset Type", values = c("Training" = "blue", "Validation" = "red")) +
  ggtitle("# Total Users by Type (Training vs. Validation)") +
  theme_bw()

# Function to calculate statistics
calculate_statistics <- function(x) {
  return(c(
    Min = min(x, na.rm = TRUE),
    Q1 = quantile(x, 0.25, na.rm = TRUE),
    Median = median(x, na.rm = TRUE),
    Mean = mean(x, na.rm = TRUE),
    Q3 = quantile(x, 0.75, na.rm = TRUE),
    Max = max(x, na.rm = TRUE)
  ))
}


```

### Histogram for Number of Bikes Rented

The Histogram shows that the distribution of cnt in training dataset shows a bimodal shape which has two peaks, while the validation shows left skewed with only one peak.

```{r}
par(mfrow = c(1, 2))
hist(training_data$cnt, main = "Training Set Histogram", xlab = "# bikes rented", col = "blue")
hist(validation_data$cnt, main = "Validation Set Histogram", xlab = "# bikes rented", col = "red")
par(mfrow = c(1, 1))
```

### Histogram for Number of Registered Users and Sqrt_Casual Users

The histograms for registered users are similarly normally distributed, and both are slightly right skewed. The histogram of sqrt(casual) users for each data set shows similarity.

```{r}
#for registered
par(mfrow = c(1, 2))
hist(training_data$registered, main = "Training Set Histogram", xlab = "registered users", col = "blue")
hist(validation_data$registered, main = "Validation Set Histogram", xlab = "registered users", col = "red")
par(mfrow = c(1, 1))

#for casual
par(mfrow = c(1, 2))
hist(sqrt(training_data$casual), main = "Training Set Histogram", xlab = "sqrt casual users", col = "blue")
hist(sqrt(validation_data$casual), main = "Validation Set Histogram", xlab = "sqrt casual users", col = "red")
par(mfrow = c(1, 1))
```

### Categorical Predictors: [weather, season, workingday] Continuous Predictors: [temperature, feeling temperature, wind speed, humidity]

#### Counts and Proportions For categorical

The categorical variables in the two data sets have similar counts and proportions.

```{r}
#model1 <- lm(registered~as.factor(workingday) + as.factor(yr) + as.factor(weather))
#model2 <- lm(sqrt(casual)~as.factor(season) + as.factor(yr) + as.factor(weather) + as.factor(workingday))
#model3 <- lm(cnt ~  temp + wind + hum + as.factor(yr) + as.factor(workingday))

# WorkingDay
training_counts_workingday <- table(training_data$workingday)
validation_counts_workingday <- table(validation_data$workingday)

training_props_workingday <- prop.table(training_counts_workingday)
validation_props_workingday <- prop.table(validation_counts_workingday)

# Season
training_counts_season <- table(training_data$season)
validation_counts_season <- table(validation_data$season)

training_props_season <- prop.table(training_counts_season)
validation_props_season <- prop.table(validation_counts_season)

# Year
training_counts_yr <- table(training_data$yr)
validation_counts_yr <- table(validation_data$yr)

training_props_yr <- prop.table(training_counts_yr)
validation_props_yr <- prop.table(validation_counts_yr)

# Holiday
training_counts_holiday <- table(training_data$holiday)
validation_counts_holiday <- table(validation_data$holiday)

training_props_holiday <- prop.table(training_counts_holiday)
validation_props_holiday <- prop.table(validation_counts_holiday)

# Weathersit
training_counts_weathersit <- table(training_data$weathersit)
validation_counts_weathersit <- table(validation_data$weathersit)

training_props_weathersit <- prop.table(training_counts_weathersit)
validation_props_weathersit <- prop.table(validation_counts_weathersit)

```

```{r}
# Printing out the tables
# For a better layout, you might want to print these one by one or use a custom layout with a package like gridExtra
#working day
cat("Training Set - Working Day Count and Proportion")
print(training_counts_workingday)
print(training_props_workingday)

cat("\nValidation Set - Working Day Count and Proportion")
print(validation_counts_workingday)
print(validation_props_workingday)
cat("\n")
#season
cat("Training Set - Season Count and Proportion")
print(training_counts_season)
print(training_props_season)

cat("\nValidation Set - Season Count and Proportion")
print(validation_counts_season)
print(validation_props_season)
cat("\n")
#year
cat("Training Set - Year Count and Proportion")
print(training_counts_yr)
print(training_props_yr)

cat("\nValidation Set - Year Count and Proportion")
print(validation_counts_yr)
print(validation_props_yr)
cat("\n")
#holiday
cat("Training Set - Holiday Count and Proportion")
print(training_counts_holiday)
print(training_props_holiday)

cat("\nValidation Set - Holiday Count and Proportion")
print(validation_counts_holiday)
print(validation_props_holiday)
cat("\n")
#weathersit
cat("Training Set - Weathersit Count and Proportion")
print(training_counts_weathersit)
print(training_props_weathersit)

cat("\nValidation Set - Weathersit Count and Proportion")
print(validation_counts_weathersit)
print(validation_props_weathersit)
```

#### Histograms For continuous Predictors

All four continuous variables are similarly distributed in the two data sets. In general, the temperature in 2012 is higher than in 2011, and the humidity is relatively lower and more widely spread in 2012 than that in 2011. Also, the wind speed is generally higher in 2012.

```{r}
#For temp
par(mfrow = c(1, 2))
hist(training_data$temp, main = "Training Set Histogram", xlab = "temperature", col = "blue")
hist(validation_data$temp, main = "Validation Set Histogram", xlab = "temperature", col = "red")
par(mfrow = c(1, 1))

#For feeling temp
par(mfrow = c(1, 2))
hist(training_data$atemp, main = "Training Set Histogram", xlab = "feeling temperature", col = "blue")
hist(validation_data$atemp, main = "Validation Set Histogram", xlab = "feeling temperature", col = "red")
par(mfrow = c(1, 1))

#For hum
par(mfrow = c(1, 2))
hist(training_data$hum, main = "Training Set Histogram", xlab = "humidity", col = "blue")
hist(validation_data$hum, main = "Validation Set Histogram", xlab = "humidity", col = "red")
par(mfrow = c(1, 1))

#For windspeed
par(mfrow = c(1, 2))
hist(training_data$windspeed, main = "Training Set Histogram", xlab = "windspeed", col = "blue")
hist(validation_data$windspeed, main = "Validation Set Histogram", xlab = "windspeed", col = "red")
par(mfrow = c(1, 1))
```

# Model Fitting & obtain outputs of the model

## Full models for each question

We are fitting full model which will be used to stepwise regression later. We address multicollinearity issue and delete variables that might cause it.

### Viki trail 1

```{r}
#viki trail 1

full_model1 <- lm(registered ~ as.factor(workingday) + as.factor(season)+ holiday + weekday + temp + hum + windspeed + as.factor(weathersit),
                 data = training_data)

full_model2 <- lm( sqrt(casual)~ as.factor(workingday) + as.factor(season) + holiday + weekday + temp + hum + windspeed + as.factor(weathersit),
                 data = training_data)

full_model3 <- lm( cnt ~ casual +as.factor(workingday) + as.factor(season) + holiday + weekday + temp + hum + windspeed + as.factor(weathersit),
                 data = training_data)

summary(full_model1)
summary(full_model2)
summary(full_model3)

#plot added variables
avPlots(full_model1)
avPlots(full_model2)
avPlots(full_model3)
```

# Stepwise Regression

```{r}
stepwise_model1 <- step(full_model1, direction = "both")
stepwise_model2 <- step(full_model2, direction = "both")
stepwise_model3 <- step(full_model3, direction = "both")
```

```{r}
summary(stepwise_model1)
summary(stepwise_model2)
summary(stepwise_model3)
```

#### model 1: R\^2 = 0.8075

#### model 2: R\^2 = 0.8016

#### model 3: R\^2 = 0.8928

```{r}
avPlots(stepwise_model1)
avPlots(stepwise_model2)
avPlots(stepwise_model3)
```

For vif, values close to 1 indicate no multicollinearity, values between 1-3 indicate mild collinearity which is not ideal but can be considered ok since it won't severaly impact the interpretability of the coefficients, values above indicate severe multicollinearity which is considered unreliable.

```{r}
vif(stepwise_model1)
vif(stepwise_model2)
vif(stepwise_model3)
```

# Validation

```{r}
validation_data$predicted_registered <- predict(stepwise_model1, newdata = validation_data)
validation_data$predicted_sqrtcasual <- predict(stepwise_model2, newdata = validation_data)
validation_data$predicted_count <- predict(stepwise_model3, newdata = validation_data)
```

### Note: here in the predicitons I am unsure why the firts one is negative, in thesecond quetsion, I am unsure whether I should square the original value or not (not sure wthere it predicts the squared casaul count or not). In the third question, it produces Nans???

```{r}
observed_values1 <- validation_data$registered
predicted_values1 <- validation_data$predicted_registered
# Calculate different prediction performance metrics
# Functions from the MLmetrics package
# Common regression metrics
# Calculate the Root Mean Squared Error (RMSE), which measures the ...
# ... average magnitude of prediction errors.
# Lower is better.
rmse1 <- RMSE(predicted_values1, observed_values1)
# same as
# sqrt(mean((observed_values - predicted_values)^2))

# Compute the Mean Absolute Error (MAE), indicating the average absolute ...
# ... difference between predicted and observed values.
# Lower is better.
mae1 <- MAE(predicted_values1, observed_values1)
# same as
# mean(abs(observed_values - predicted_values))

# Calculate the Mean Absolute Percentage Error (MAPE), measuring the ...
# ... average percentage difference between predicted and observed values.
mape1 <- MAPE(predicted_values1, observed_values1)
# same as
# mean(abs(predicted_values-observed_values)/observed_values)

# Determine the R-squared (R²) Score, representing the proportion of the ...
# ... variance in the observed values (of validation data set) ... 
# ... explained by the predicted values from the model.
# Higher is better.
r_squared1 <- R2_Score(predicted_values1, observed_values1)
# same as
# summary(lm(observed_values ~ predicted_values))$r.squared

# Display the calculated metrics
cat("Root Mean Squared Error (RMSE):", round(rmse1, digits = 4), "\n")
cat("Mean Absolute Error (MAE):", round(mae1, digits = 4), "\n")
cat("R-squared (R^2) Score:", round(r_squared1, digits = 4), "\n")
cat("Mean Absolute Percentage Error (MPE):", round(mape1, digits = 4), "\n")



#second question
observed_values2 <- sqrt(validation_data$casual) #should I transform it here??
predicted_values2 <- validation_data$predicted_sqrtcasual
rmse2 <- RMSE(predicted_values2, observed_values2)
mae2 <- MAE(predicted_values2, observed_values2)
mape2 <- MAPE(predicted_values2, observed_values2)
r_squared2 <- R2_Score(predicted_values2, observed_values2)
cat("Root Mean Squared Error (RMSE):", round(rmse2, digits = 4), "\n")
cat("Mean Absolute Error (MAE):", round(mae2, digits = 4), "\n")
cat("R-squared (R^2) Score:", round(r_squared2, digits = 4), "\n")
cat("Mean Absolute Percentage Error (MPE):", round(mape2, digits = 4), "\n")



#third question
observed_values3 <- validation_data$count
predicted_values3 <- validation_data$predicted_count
rmse3 <- RMSE(predicted_values3, observed_values3)
mae3 <- MAE(predicted_values3, observed_values3)
mape3 <- MAPE(predicted_values3, observed_values3)
r_squared3 <- R2_Score(predicted_values3, observed_values3)
cat("Root Mean Squared Error (RMSE):", round(rmse3, digits = 4), "\n")
cat("Mean Absolute Error (MAE):", round(mae3, digits = 4), "\n")
cat("R-squared (R^2) Score:", round(r_squared3, digits = 4), "\n")
cat("Mean Absolute Percentage Error (MPE):", round(mape3, digits = 4), "\n")


```

#### not working for model 3??

## Diagnostics of Training model

we **choose** model 1 because it has best RMSE for validation

```{r}
#stepwise regression on model 1
m.mlr <- lm(registered ~ as.factor(workingday) + as.factor(season) +
    temp + windspeed + as.factor(weathersit),
                 data = training_data)

#original regression question 1
m.mlr1 <- lm(registered ~ as.factor(workingday) + as.factor(weathersit),
                 data = training_data)

#question2
m.mlr2 <- lm(sqrt(casual) ~ as.factor(workingday) + as.factor(season) + holiday + 
  temp + hum + windspeed + as.factor(weathersit),
                 data = training_data)


```

```{r}
diagnostics_df <- data.frame(
  Residuals = resid(m.mlr),
  Fitted_Values = fitted(m.mlr),
  Standardized_Residuals = rstandard(m.mlr),
  Leverage = hatvalues(m.mlr)
  #Date = year2011$dteday
)

# Create the standardized residuals vs. fitted values plot
ggplot(diagnostics_df, aes(x = Fitted_Values, y = Residuals)) +
  geom_point(col="blue", alpha=0.75) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs( title = "Residuals vs. Fitted Values",
        x = "Fitted Values", y = "Residuals") +
  theme_bw()

# Create the QQ plot
ggplot(diagnostics_df, aes(sample = Standardized_Residuals)) +
  stat_qq(aes(sample = Standardized_Residuals), distribution = qnorm,
          size = 2, col="blue", alpha = 0.75) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Standardized Residual QQ Plot",
       x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_bw()

# Create the sqrt(|standardized residuals|) vs. fitted values plot
ggplot(diagnostics_df, aes(x = Fitted_Values, y = sqrt(abs(Standardized_Residuals)))) +
  geom_point(col="blue", alpha=0.75) +
  labs( title = "Residuals vs. Fitted Values",
        x = "Fitted Values", y = "sqrt(|Standardized Residuals|)") +
  theme_bw()

# Leverage vs Standardized Residuals
ggplot(diagnostics_df, aes(x = Leverage, y = Standardized_Residuals)) +
  geom_point(alpha = 0.75) +
  labs(title = "Standardized Residuals vs. Leverage Plot",
       x = "Leverage", y = "Standardized Residuals") +
  theme_bw()

```

# Prediction

```{r}
validation2012<- subset(validation_data)
validation2012$predicted_registered <- predict(m.mlr, newdata = validation2012)
summary(validation2012$predicted_registered)

```

```{r}
# Extract observed and predicted values
observed_values <- validation2012$registered
predicted_values <- validation2012$predicted_registered
rmse <- RMSE(predicted_values, observed_values)
mae <- MAE(predicted_values, observed_values)
mape <- MAPE(predicted_values, observed_values)
r_squared <- R2_Score(predicted_values, observed_values)


cat("Root Mean Squared Error (RMSE):", round(rmse, digits = 4), "\n")
cat("Mean Absolute Error (MAE):", round(mae, digits = 4), "\n")
cat("R-squared (R^2) Score:", round(r_squared, digits = 4), "\n")
cat("Mean Absolute Percentage Error (MPE):", round(mape, digits = 4), "\n")
```

```{r}
ggplot(validation2012, aes(x = observed_values, y = predicted_values)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(x = "Observed Values", y = "Predicted Values",
       title = "Observed vs. Predicted Values") +
  theme_bw()

# Residuals plot
ggplot(validation2012, aes(x = 1:nrow(validation2012), y = observed_values-predicted_values)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 0, color = "red", linetype = "dashed") +
  labs(x = "Observation Index", y = "Residuals",
       title = "Observed vs. Predicted Values") +
  theme_bw()

```

### Kaylee Trial 1

```{r}
#kaylee trial 1

full_model1 <- lm(registered ~ as.factor(workingday) + as.factor(season)+ as.factor(holiday) + as.factor(weekday) + temp + hum + windspeed + as.factor(weathersit),
                 data = training_data)

full_model2 <- lm( sqrt(casual)~ as.factor(workingday) + as.factor(season)+ as.factor(holiday) + as.factor(weekday) + temp + hum + windspeed + as.factor(weathersit),
                 data = training_data)

full_model3 <- lm( cnt ~ casual+as.factor(workingday) + as.factor(season)+ as.factor(holiday) + as.factor(weekday) + temp + hum + windspeed + as.factor(weathersit),
                 data = training_data)

summary(full_model1)
summary(full_model2)
summary(full_model3)

#plot added variables
avPlots(full_model1)
avPlots(full_model2)
avPlots(full_model3)
```

Compared with Viki's first trial, the AIC for model2 improved as well.(1151 to 1147)

```{r}
stepwise_model1 <- step(full_model1, direction = "both")
stepwise_model2 <- step(full_model2, direction = "both")
stepwise_model3 <- step(full_model3, direction = "both")
```

```{r}
summary(stepwise_model1)
summary(stepwise_model2)
summary(stepwise_model3)
```

```{r}
avPlots(stepwise_model1)
avPlots(stepwise_model2)
avPlots(stepwise_model3)
```

```{r}
vif(stepwise_model1)
vif(stepwise_model2)
vif(stepwise_model3)
```

The above vif is the result after doing following changes: Because of high multicollinearity of workingday and weekday in model2, we will remove one of them from model2. We also remove sqrt from sqrt(casual) in model 3, which changes back to casual, the multicollinearity reduces because GVIF of casual goes down from 4.88 to 3.57, but the R-squared for model3 reduces from 90% to 88%, which is good because effect of overfitting reduces.

### Kaylee Trial 2

```{r}
#kaylee trial 2

full_model1 <- lm(registered ~ as.factor(workingday) + as.factor(season)+ as.factor(holiday) + as.factor(weekday) + temp + hum + windspeed + as.factor(weathersit),
                 data = training_data)

full_model2 <- lm( sqrt(casual)~ as.factor(workingday) + as.factor(season)+ temp + hum + windspeed + as.factor(weathersit),
                 data = training_data)

full_model3 <- lm( cnt ~ casual+as.factor(workingday) + as.factor(season)+ as.factor(holiday) + temp + hum + windspeed + as.factor(weathersit),
                 data = training_data)

summary(full_model1)
summary(full_model2)
summary(full_model3)

#plot added variables
avPlots(full_model1)
avPlots(full_model2)
avPlots(full_model3)
```

```{r}
#kaylee
stepwise_model1 <- step(full_model1, direction = "both")
stepwise_model2 <- step(full_model2, direction = "both")
stepwise_model3 <- step(full_model3, direction = "both")
```

```{r}
summary(stepwise_model1)
summary(stepwise_model2)
summary(stepwise_model3)
```

```{r}
avPlots(stepwise_model1)
avPlots(stepwise_model2)
avPlots(stepwise_model3)
```

```{r}
vif(stepwise_model1)
vif(stepwise_model2)
vif(stepwise_model3)
```

# Validation

```{r}
validation_data$predicted_registered <- predict(stepwise_model1, newdata = validation_data)
validation_data$predicted_sqrtcasual <- predict(stepwise_model2, newdata = validation_data)
validation_data$predicted_count <- predict(stepwise_model3, newdata = validation_data)
```

### Note: In the third question, it produces Nans???

```{r}
observed_values1 <- validation_data$registered
predicted_values1 <- validation_data$predicted_registered
# Calculate different prediction performance metrics
# Functions from the MLmetrics package
# Common regression metrics
# Calculate the Root Mean Squared Error (RMSE), which measures the ...
# ... average magnitude of prediction errors.
# Lower is better.
rmse1 <- RMSE(predicted_values1, observed_values1)
# same as
# sqrt(mean((observed_values - predicted_values)^2))

# Compute the Mean Absolute Error (MAE), indicating the average absolute ...
# ... difference between predicted and observed values.
# Lower is better.
mae1 <- MAE(predicted_values1, observed_values1)
# same as
# mean(abs(observed_values - predicted_values))

# Calculate the Mean Absolute Percentage Error (MAPE), measuring the ...
# ... average percentage difference between predicted and observed values.
mape1 <- MAPE(predicted_values1, observed_values1)
# same as
# mean(abs(predicted_values-observed_values)/observed_values)

# Determine the R-squared (R²) Score, representing the proportion of the ...
# ... variance in the observed values (of validation data set) ... 
# ... explained by the predicted values from the model.
# Higher is better.
r_squared1 <- R2_Score(predicted_values1, observed_values1)
# same as
# summary(lm(observed_values ~ predicted_values))$r.squared

# Display the calculated metrics
cat("Root Mean Squared Error (RMSE):", round(rmse1, digits = 4), "\n")
cat("Mean Absolute Error (MAE):", round(mae1, digits = 4), "\n")
cat("R-squared (R^2) Score:", round(r_squared1, digits = 4), "\n")
cat("Mean Absolute Percentage Error (MPE):", round(mape1, digits = 4), "\n")



#second question
observed_values2 <- sqrt(validation_data$casual) #viki: should I transform it here?? kaylee: I think so
predicted_values2 <- validation_data$predicted_sqrtcasual
rmse2 <- RMSE(predicted_values2, observed_values2)
mae2 <- MAE(predicted_values2, observed_values2)
mape2 <- MAPE(predicted_values2, observed_values2)
r_squared2 <- R2_Score(predicted_values2, observed_values2)
cat("Root Mean Squared Error (RMSE):", round(rmse2, digits = 4), "\n")
cat("Mean Absolute Error (MAE):", round(mae2, digits = 4), "\n")
cat("R-squared (R^2) Score:", round(r_squared2, digits = 4), "\n")
cat("Mean Absolute Percentage Error (MPE):", round(mape2, digits = 4), "\n")



#third question
observed_values3 <- validation_data$count
predicted_values3 <- validation_data$predicted_count
rmse3 <- RMSE(predicted_values3, observed_values3)
mae3 <- MAE(predicted_values3, observed_values3)
mape3 <- MAPE(predicted_values3, observed_values3)
r_squared3 <- R2_Score(predicted_values3, observed_values3)
cat("Root Mean Squared Error (RMSE):", round(rmse3, digits = 4), "\n")
cat("Mean Absolute Error (MAE):", round(mae3, digits = 4), "\n")
cat("R-squared (R^2) Score:", round(r_squared3, digits = 4), "\n")
cat("Mean Absolute Percentage Error (MPE):", round(mape3, digits = 4), "\n")


```

#### model1 R\^2 = 80%, R\^2 score = -0.84, RMSE = 1931

#### model2 R\^2 = 79%, R\^2 score = 0.5814, RMSE = 7.64

#### model3 R\^3 = 89%, R\^2 score = NaN, RMSE = NaN

#### not working for model 3??

## Diagnostics of Training model

we **choose** model 2 because it has best RMSE and R\^2 (it is done for whole 2011 year)

```{r}
m.mlr <- lm( sqrt(casual)~ as.factor(workingday) + as.factor(season)+ temp + hum + windspeed + as.factor(weathersit),
                 data = training_data)

```

```{r}
diagnostics_df <- data.frame(
  Residuals = resid(m.mlr),
  Fitted_Values = fitted(m.mlr),
  Standardized_Residuals = rstandard(m.mlr),
  Leverage = hatvalues(m.mlr)
  #Date = year2011$dteday
)

# Create the standardized residuals vs. fitted values plot
ggplot(diagnostics_df, aes(x = Fitted_Values, y = Residuals)) +
  geom_point(col="blue", alpha=0.75) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs( title = "Residuals vs. Fitted Values",
        x = "Fitted Values", y = "Residuals") +
  theme_bw()

# Create the QQ plot
ggplot(diagnostics_df, aes(sample = Standardized_Residuals)) +
  stat_qq(aes(sample = Standardized_Residuals), distribution = qnorm,
          size = 2, col="blue", alpha = 0.75) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Standardized Residual QQ Plot",
       x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_bw()

# Create the sqrt(|standardized residuals|) vs. fitted values plot
ggplot(diagnostics_df, aes(x = Fitted_Values, y = sqrt(abs(Standardized_Residuals)))) +
  geom_point(col="blue", alpha=0.75) +
  labs( title = "Residuals vs. Fitted Values",
        x = "Fitted Values", y = "sqrt(|Standardized Residuals|)") +
  theme_bw()

# Leverage vs Standardized Residuals
ggplot(diagnostics_df, aes(x = Leverage, y = Standardized_Residuals)) +
  geom_point(alpha = 0.75) +
  labs(title = "Standardized Residuals vs. Leverage Plot",
       x = "Leverage", y = "Standardized Residuals") +
  theme_bw()

```

# Prediction

```{r}
validation2012<- subset(validation_data)
validation2012$predicted_casual <- predict(m.mlr, newdata = validation2012)
summary(validation2012$predicted_casual)

```

```{r}
# Extract observed and predicted values
observed_values <- sqrt(validation2012$casual)
predicted_values <- validation2012$predicted_casual
rmse <- RMSE(predicted_values, observed_values)
mae <- MAE(predicted_values, observed_values)
mape <- MAPE(predicted_values, observed_values)
r_squared <- R2_Score(predicted_values, observed_values)


cat("Root Mean Squared Error (RMSE):", round(rmse, digits = 4), "\n")
cat("Mean Absolute Error (MAE):", round(mae, digits = 4), "\n")
cat("R-squared (R^2) Score:", round(r_squared, digits = 4), "\n")
cat("Mean Absolute Percentage Error (MPE):", round(mape, digits = 4), "\n")
```

```{r}
ggplot(validation2012, aes(x = observed_values, y = predicted_values)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(x = "Observed Values", y = "Predicted Values",
       title = "Observed vs. Predicted Values") +
  theme_bw()


# Residuals plot
ggplot(validation2012, aes(x = 1:nrow(validation2012), y = observed_values-predicted_values)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 0, color = "red", linetype = "dashed") +
  labs(x = "Observation Index", y = "Residuals",
       title = "Observed vs. Predicted Values") +
  theme_bw()


```

# Fit AR(1) Model for the model2 we chose

```{r}
head(training_data$dteday)
```

```{r}
# Create a new data frame 'tempdataset' by selecting specific columns from 'training_data'
# For 'Date' column: training_data$dteday contains the date in "dd/mm/yyyy" format.
# We use the as.Date function and specify the format as "%d/%m/%Y" to match ...
tempdataset <- data.frame(Date = as.Date(training_data$dteday, format = "%d/%m/%Y"),
                          CasualUsers = training_data$casual,
                          WorkingDay = training_data$workingday,
                          Season = training_data$season,
                          Temperature = training_data$temp,
                          Humidity = training_data$hum,
                          WindSpeed = training_data$windspeed,
                          Weathersit = training_data$weathersit)

# Ensure there are no missing values
Dataset <- tempdataset[complete.cases(tempdataset), ]

# Remove the 'tempdataset' data frame from the R environment
rm(tempdataset)

# Add a square root transformation of the CasualUsers column
Dataset$SqrtCasualUsers <- sqrt(Dataset$CasualUsers)

# Fit the model
m.mlr <- lm(SqrtCasualUsers ~ as.factor(WorkingDay) + as.factor(Season) + 
            Temperature + Humidity + WindSpeed + as.factor(Weathersit), 
            data = Dataset)

# Check the model's summary
summary(m.mlr)
```

```{r}
library(ggplot2)
# Time series modeling
# Create a variable indicating observations ordered by time
Dataset$Time_Seq <- order(Dataset$Date)

# Explore the correlation between GroundCO and Time
ggplot(Dataset, aes(x = Time_Seq, y = SqrtCasualUsers)) +
  geom_point(color = 'blue', alpha = 0.75) +
  geom_abline(intercept = 0, slope = 0, color = "red", linetype = "dashed") +
  labs(x = "Time", y = "SqrtCasualUsers", title = "Time vs. SqrtCasualUsers") +
  theme_bw()
```

Based on the plot of the relationship between the transformed response variable - the square root of casual users - and the Time, such transformation helps in stabilizing the variance and makes the data more normally distributed.

```{r}
# Perform Multiple Linear Regression with Time_Seq as a predictor
m.mlr.time <- lm(SqrtCasualUsers ~ as.factor(WorkingDay) + as.factor(Season) + 
            Temperature + Humidity + WindSpeed + as.factor(Weathersit) + Time_Seq, data = Dataset)

# Examine the model fit for m.mlr.time
summary(m.mlr.time)

#get the AIC BIC R^2
# Summary of the model
summary_mmlr_time <- summary(m.mlr.time)

# Extract AIC and BIC
aic_mmlr_time <- AIC(m.mlr.time)
bic_mmlr_time <- BIC(m.mlr.time)

# Extract R-squared and Adjusted R-squared
rsquared_mmlr_time <- summary_mmlr_time$r.squared
adjrsquared_mmlr_time <- summary_mmlr_time$adj.r.squared

# Print the AIC, BIC, R-squared and Adjusted R-squared
cat("AIC:", aic_mmlr_time, "\n")
cat("BIC:", bic_mmlr_time, "\n")
cat("R-squared:", rsquared_mmlr_time, "\n")
cat("Adjusted R-squared:", adjrsquared_mmlr_time, "\n")
```

```{r}
# Time Series: Autocorrelation function
# We shall now study the correlation of the response with respect to time
#   - Dataset$SqrtCasualUsers: Time series data for which autocorrelation is calculated
#   - lag.max: Maximum number of lags to compute autocorrelation (default is often set to 10)
#   - plot: Boolean, whether to plot the autocorrelation function (TRUE or FALSE)
acf(Dataset$SqrtCasualUsers, lag.max = 100, plot = TRUE)
acf(residuals(m.mlr.time)[Dataset$Time_Seq], lag.max = 100, plot = TRUE)
```

In summary, this ACF plot suggests that there is some autocorrelation in the early lags of the **`SqrtCasualUsers`** time series, but it quickly diminishes, implying that recent past values (especially the immediate last value) are somewhat informative in predicting the current value, but this influence diminishes as you go further back in time. We will consider GLS with AR(1) because we have multiple predictors, and there may be autocorrelation in the residuals of a multiple regression model, we will use a GLS model with AR(1) correlation.

## GLS model with AR(1)

```{r}
#install.packages("nlme")
```

```{r}
library(nlme)
# Run Generalized Least Squares (GLS) with corAR1 model (Auto-Regressive 1)
# Note: This might take some time, depending on your computer and memory
m.gls <- gls(SqrtCasualUsers ~ as.factor(WorkingDay) + as.factor(Season) + Temperature + Humidity + WindSpeed + as.factor(Weathersit),
              data = Dataset,
              correlation = corAR1(form = ~ Time_Seq),
              method = "ML")  # Maximum Likelihood estimation

# Display summary information about the GLS model
summary(m.gls)

```

```{r}
# Diagnostics and plots for the time series model
# Autocorrelation plot of residuals
acf(residuals(m.gls))
```

### Model Fit

```{r}
# Calculate predictions
predictions <- fitted(m.gls)

# Calculate residuals
residuals <- Dataset$SqrtCasualUsers - predictions

# Calculate RMSE
rmse <- sqrt(mean(residuals^2))

# Extract log-likelihood to compute AIC and BIC
loglik <- logLik(m.gls)

# Calculate AIC
aic <- AIC(m.gls)

# Calculate BIC
bic <- BIC(m.gls)

# Output the results
list(RMSE = rmse, AIC = aic, BIC = bic)

######For the R-squared and Adjusted R-squared
# Total Sum of Squares
SST <- sum((Dataset$SqrtCasualUsers - mean(Dataset$SqrtCasualUsers))^2)

# Residual Sum of Squares
RSS <- sum(residuals^2)

# R-squared
r_squared <- 1 - (RSS/SST)

# Adjusted R-squared
n <- nrow(Dataset)  # Number of observations
p <- length(coef(m.gls))  # Number of predictors including intercept
adj_r_squared <- 1 - ((1 - r_squared) * (n - 1) / (n - p - 1))

# Output R-squared and Adjusted R-squared
list(R_squared = r_squared, Adjusted_R_squared = adj_r_squared)
```

### Model Diagnostics

```{r}
library(nlme)
#Extract Residuals and Fitted Values:
fitted_vals <- fitted(m.gls)
residuals_vals <- residuals(m.gls)
plot(fitted_vals, residuals_vals,
     xlab = "Fitted Values",
     ylab = "Residuals",
     main = "Residuals vs Fitted")
abline(h = 0, col = "red", lty = 2)

#QQ plot for standardized residual plot
qqnorm(residuals_vals, main = "Q-Q Residuals")
qqline(residuals_vals, col = "red", lty = 2)

#Scale-Location Plot (Spread vs Level):
# First, ensure there are no missing values in the residuals or the fitted values
resid <- residuals(m.gls, type = "pearson")  # Pearson residuals are standardized
fitted_vals <- fitted(m.gls)
# Since we're plotting sqrt of abs(residuals), ensure no NAs are produced in the process
# Calculate the square root of the absolute standardized residuals
sqrt_abs_std_resid <- sqrt(abs(resid))
# Now create the Scale-Location plot
plot(fitted_vals, sqrt_abs_std_resid,
     xlab = "Fitted Values",
     ylab = "Sqrt of Absolute Standardized Residuals",
     main = "Scale-Location")
abline(h = 0, col = "red", lty = 2)

```

### Model Prediction


```{r}
Vtempdataset <- data.frame(Date = as.Date(validation_data$dteday, format = "%d/%m/%Y"),
                          CasualUsers = validation_data$casual,
                          WorkingDay = validation_data$workingday,
                          Season = validation_data$season,
                          Temperature = validation_data$temp,
                          Humidity = validation_data$hum,
                          WindSpeed = validation_data$windspeed,
                          Weathersit = validation_data$weathersit)

# Ensure there are no missing values
VDataset <- Vtempdataset[complete.cases(Vtempdataset), ]

# Remove the 'tempdataset' data frame from the R environment
rm(Vtempdataset)

# Add a square root transformation of the CasualUsers column
VDataset$SqrtCasualUsers <- sqrt(VDataset$CasualUsers)
library(ggplot2)
# Time series modeling
# Create a variable indicating observations ordered by time
VDataset$Time_Seq <- order(VDataset$Date)

# Explore the correlation between GroundCO and Time
ggplot(VDataset, aes(x = Time_Seq, y = SqrtCasualUsers)) +
  geom_point(color = 'blue', alpha = 0.75) +
  geom_abline(intercept = 0, slope = 0, color = "red", linetype = "dashed") +
  labs(x = "Time", y = "SqrtCasualUsers", title = "Time vs. SqrtCasualUsers") +
  theme_bw()
```


```{r}
# Predict using the validation dataset
VDataset$PredictedSqrtCasualUsers <- predict(m.gls, newdata = VDataset)

# Plot of fitted values against observed square root values from the validation dataset
plot(VDataset$PredictedSqrtCasualUsers, VDataset$SqrtCasualUsers, 
     xlab = "Predicted Fitted Values", ylab = "Observed Sqrt Casual Users",
     main = "Predicted vs. Observed Values")
abline(a = 0, b = 1, col = "red", lty = 2)

# For plotting residuals over time, you need to calculate residuals for the validation dataset.
# Residuals are the differences between the observed and predicted values.
VDataset$Residuals <- VDataset$SqrtCasualUsers - VDataset$PredictedSqrtCasualUsers

# Plot of residuals over time
# Assuming you have a time sequence variable in your validation dataset named Time_Seq
plot(VDataset$Time_Seq, VDataset$Residuals, 
     xlab = "Time", ylab = "Residuals",
     main = "Residuals over Time")
```



# For Regression Model we got from Report 3

## Model Fitting & obtain outputs of the model

```{r}
model1 <- lm(registered ~ as.factor(workingday)+ as.factor(weathersit) + temp + windspeed,
             data = training_data)
summary(model1) #r^2 = 0.6532 
#model2
model2 <- lm(sqrt(casual)~as.factor(season) + as.factor(weathersit) + as.factor(workingday),data = training_data)
summary(model2) #r^2 = 0.6873 
#model3
model3 <- lm(cnt ~  temp + windspeed + hum  + as.factor(workingday),data = training_data)
summary(model3) #r^2 = 0.6511 
```

```{r}
#Model Diagnostics: to assess assumptions and multicollinearity

#!!!Model1!!!
# Plotting added variable plots
avPlots(model1)
# Check for multicollinearity: computing variance inflation factors (VIFs)
vif(model1)
# Create a data frame with the residuals and fitted values
diagnostics_df1 <- data.frame(Residuals1 = resid(model1),
                             Fitted_Values1 = fitted(model1),
                             Standardized_Residuals1 = rstandard(model1),
                             Leverage1 = hatvalues(model1),
                             Season = training_data$season,
                             Workingday = training_data$workingday,
                             Weather = training_data$weathersit)

#!!!Model2!!!
avPlots(model2)
vif(model2)
diagnostics_df2 <- data.frame(Residuals2 = resid(model2),
                              Fitted_Values2 = fitted(model2),
                              Standardized_Residuals2 = rstandard(model2),
                              Leverage2 = hatvalues(model2),
                              Season = training_data$season,
                            Workingday = training_data$workingday,
                             Weather = training_data$weathersit)

#!!!Model3!!!
avPlots(model3)
vif(model3)
diagnostics_df3 <- data.frame(Residuals3 = resid(model3),
                              Fitted_Values3 = fitted(model3),
                              Standardized_Residuals3 = rstandard(model3),
                              Leverage3 = hatvalues(model3),
                              Season = training_data$season,
                              Workingday = training_data$workingday,
                             Weather = training_data$weathersit)
```

plot the diagnostics

```{r}
#!!!Model1!!!
# Create the standardized residuals vs. fitted values plot
ggplot(diagnostics_df1, aes(x = Fitted_Values1, y = Residuals1)) +
  geom_point(col="blue", alpha=0.75) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs( title = "Residuals vs. Fitted Values",
        x = "Fitted Values", y = "Residuals") +
  theme_bw()
# Create the QQ plot
ggplot(diagnostics_df1, aes(sample = Standardized_Residuals1)) +
  stat_qq(aes(sample = Standardized_Residuals1), distribution = qnorm,
          size = 2, col="blue", alpha = 0.75) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Standardized Residual QQ Plot",
       x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_bw()
# Create the sqrt(|standardized residuals|) vs. fitted values plot
ggplot(diagnostics_df1, aes(x = Fitted_Values1, y = sqrt(abs(Standardized_Residuals1)))) +
  geom_point(col="blue", alpha=0.75) +
  labs( title = "Residuals vs. Fitted Values",
        x = "Fitted Values", y = "sqrt(|Standardized Residuals|)") +
  theme_bw()
# Leverage vs Standardized Residuals
ggplot(diagnostics_df1, aes(x = Leverage1, y = Standardized_Residuals1)) +
  geom_point(alpha = 0.75) +
  labs(title = "Standardized Residuals vs. Leverage Plot",
       x = "Leverage", y = "Standardized Residuals") +
  theme_bw()

```

```{r}
#!!!Model2!!!
#residual vs fitted values
ggplot(diagnostics_df2, aes(x = Fitted_Values2, y = Residuals2)) +
  geom_point(col="blue", alpha=0.75) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs( title = "Residuals vs. Fitted Values",
        x = "Fitted Values", y = "Residuals") +
  theme_bw()
#QQ plot
ggplot(diagnostics_df2, aes(sample = Standardized_Residuals2)) +
  stat_qq(aes(sample = Standardized_Residuals2), distribution = qnorm,
          size = 2, col="blue", alpha = 0.75) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Standardized Residual QQ Plot",
       x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_bw()
#sqrt(|standardized residuals|) vs. fitted values plot
ggplot(diagnostics_df2, aes(x = Fitted_Values2, y = sqrt(abs(Standardized_Residuals2)))) +
  geom_point(col="blue", alpha=0.75) +
  labs( title = "Residuals vs. Fitted Values",
        x = "Fitted Values", y = "sqrt(|Standardized Residuals|)") +
  theme_bw()
# Leverage vs Standardized Residuals
ggplot(diagnostics_df2, aes(x = Leverage2, y = Standardized_Residuals2)) +
  geom_point(alpha = 0.75) +
  labs(title = "Standardized Residuals vs. Leverage Plot",
       x = "Leverage", y = "Standardized Residuals") +
  theme_bw()
```

```{r}
#!!!Model3!!!
#residual vs fitted values
ggplot(diagnostics_df3, aes(x = Fitted_Values3, y = Residuals3)) +
  geom_point(col="blue", alpha=0.75) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs( title = "Residuals vs. Fitted Values",
        x = "Fitted Values", y = "Residuals") +
  theme_bw()
#QQ plot
ggplot(diagnostics_df3, aes(sample = Standardized_Residuals3)) +
  stat_qq(aes(sample = Standardized_Residuals3), distribution = qnorm,
          size = 2, col="blue", alpha = 0.75) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Standardized Residual QQ Plot",
       x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_bw()
#sqrt(|standardized residuals|) vs. fitted values plot
ggplot(diagnostics_df3, aes(x = Fitted_Values3, y = sqrt(abs(Standardized_Residuals3)))) +
  geom_point(col="blue", alpha=0.75) +
  labs( title = "Residuals vs. Fitted Values",
        x = "Fitted Values", y = "sqrt(|Standardized Residuals|)") +
  theme_bw()
# Leverage vs Standardized Residuals
ggplot(diagnostics_df3, aes(x = Leverage3, y = Standardized_Residuals3)) +
  geom_point(alpha = 0.75) +
  labs(title = "Standardized Residuals vs. Leverage Plot",
       x = "Leverage", y = "Standardized Residuals") +
  theme_bw()
```

#Prediction: training and validation

#Model1 - training

```{r}
#model1
training_data$Predicted_registered <- predict(model1, training_data)
# Extract observed and predicted values
observed_values1 <- training_data$registered
predicted_values1 <- training_data$Predicted_registered
# Calculate different prediction performance metrics
#rmse
rmse1 <- RMSE(predicted_values1, observed_values1)
#mae
mae1 <- MAE(predicted_values1, observed_values1)
#mape
mape1 <- MAPE(predicted_values1, observed_values1)
#r^2
r_squared1 <- R2_Score(predicted_values1, observed_values1)
#display
cat("Root Mean Squared Error (RMSE):", round(rmse1, digits = 4), "\n")
cat("Mean Absolute Error (MAE):", round(mae1, digits = 4), "\n")
cat("R-squared (R^2) Score:", round(r_squared1, digits = 4), "\n")
cat("Mean Absolute Percentage Error (MPE):", round(mape1, digits = 4), "\n")
# Root Mean Squared Error (RMSE): 619.1167 
# Mean Absolute Error (MAE): 519.1348 
# R-squared (R^2) Score: 0.658 
# Mean Absolute Percentage Error (MPE): 0.2562 
```

#Model1 - validation

```{r}
validation_data$Predicted_registered <- predict(model1, validation_data)
# Extract observed and predicted values
observed_values1 <- validation_data$registered
predicted_values1 <- validation_data$Predicted_registered
# Calculate different prediction performance metrics
#rmse
rmse1 <- RMSE(predicted_values1, observed_values1)
#mae
mae1 <- MAE(predicted_values1, observed_values1)
#mape
mape1 <- MAPE(predicted_values1, observed_values1)
#r^2
r_squared1 <- R2_Score(predicted_values1, observed_values1)
#display
cat("Root Mean Squared Error (RMSE):", round(rmse1, digits = 4), "\n")
cat("Mean Absolute Error (MAE):", round(mae1, digits = 4), "\n")
cat("R-squared (R^2) Score:", round(r_squared1, digits = 4), "\n")
cat("Mean Absolute Percentage Error (MPE):", round(mape1, digits = 4), "\n")
# Root Mean Squared Error (RMSE): 1993.671 
# Mean Absolute Error (MAE): 1805.783 
# R-squared (R^2) Score: -0.9646 
# Mean Absolute Percentage Error (MPE): 0.5715 
```

#Model2 - training

```{r}
# Extract observed and predicted values
sqrt_casual <- sqrt(training_data$casual)
observed_values2 <- sqrt_casual
training_data$Predicted_sqrt_casual <- predict(model2, training_data)
predicted_values2 <- training_data$Predicted_sqrt_casual 

#rmse
rmse2 <- RMSE(predicted_values2, observed_values2)
#mae
mae2 <- MAE(predicted_values2, observed_values2)
#mape
mape2 <- MAPE(predicted_values2, observed_values2)
#r^2
r_squared2 <- R2_Score(predicted_values2, observed_values2)
#display
cat("Root Mean Squared Error (RMSE):", round(rmse2, digits = 4), "\n")
cat("Mean Absolute Error (MAE):", round(mae2, digits = 4), "\n")
cat("R-squared (R^2) Score:", round(r_squared2, digits = 4), "\n")
cat("Mean Absolute Percentage Error (MPE):", round(mape2, digits = 4), "\n")
# Root Mean Squared Error (RMSE): 5.8451 
# Mean Absolute Error (MAE): 4.5223 
# R-squared (R^2) Score: 0.6925 
# Mean Absolute Percentage Error (MPE): 0.2555 
```

#Model2 - validation

```{r}
# Extract observed and predicted values
sqrt_casual <- sqrt(validation_data$casual)
observed_values2 <- sqrt_casual
validation_data$Predicted_sqrt_casual <- predict(model2, validation_data)
predicted_values2 <- validation_data$Predicted_sqrt_casual 

#rmse
rmse2 <- RMSE(predicted_values2, observed_values2)
#mae
mae2 <- MAE(predicted_values2, observed_values2)
#mape
mape2 <- MAPE(predicted_values2, observed_values2)
#r^2
r_squared2 <- R2_Score(predicted_values2, observed_values2)
#display
cat("Root Mean Squared Error (RMSE):", round(rmse2, digits = 4), "\n")
cat("Mean Absolute Error (MAE):", round(mae2, digits = 4), "\n")
cat("R-squared (R^2) Score:", round(r_squared2, digits = 4), "\n")
cat("Mean Absolute Percentage Error (MPE):", round(mape2, digits = 4), "\n")
# Root Mean Squared Error (RMSE): 8.9667 
# Mean Absolute Error (MAE): 6.9653 
# R-squared (R^2) Score: 0.4235 
# Mean Absolute Percentage Error (MPE): 0.2525 
```

#Model 3 - training

```{r}
observed_values3 <- training_data$cnt
training_data$Predicted_cnt <- predict(model3, newdata = training_data)
predicted_values3 <- training_data$Predicted_cnt
#rmse
rmse3 <- RMSE(predicted_values3, observed_values3)
#mae
mae3 <- MAE(predicted_values3, observed_values3)
#mape
mape3 <- MAPE(predicted_values3, observed_values3)
#r^2
r_squared3 <- R2_Score(predicted_values2, observed_values2)
#display
cat("Root Mean Squared Error (RMSE):", round(rmse3, digits = 4), "\n")
cat("Mean Absolute Error (MAE):", round(mae3, digits = 4), "\n")
cat("R-squared (R^2) Score:", round(r_squared3, digits = 4), "\n")
cat("Mean Absolute Percentage Error (MPE):", round(mape3, digits = 4), "\n")
# Root Mean Squared Error (RMSE): 808.7555 
# Mean Absolute Error (MAE): 649.4414 
# R-squared (R^2) Score: 0.4235 
# Mean Absolute Percentage Error (MPE): 0.2749 
```

#Model 3 - validation

```{r}
observed_values3 <- validation_data$cnt
validation_data$Predicted_cnt <- predict(model3, newdata = validation_data)
predicted_values3 <- validation_data$Predicted_cnt
#rmse
rmse3 <- RMSE(predicted_values3, observed_values3)
#mae
mae3 <- MAE(predicted_values3, observed_values3)
#mape
mape3 <- MAPE(predicted_values3, observed_values3)
#r^2
r_squared3 <- R2_Score(predicted_values2, observed_values2)
#display
cat("Root Mean Squared Error (RMSE):", round(rmse3, digits = 4), "\n")
cat("Mean Absolute Error (MAE):", round(mae3, digits = 4), "\n")
cat("R-squared (R^2) Score:", round(r_squared3, digits = 4), "\n")
cat("Mean Absolute Percentage Error (MPE):", round(mape3, digits = 4), "\n")
# Root Mean Squared Error (RMSE): 2373.45 
# Mean Absolute Error (MAE): 2119.981 
# R-squared (R^2) Score: 0.4235 
# Mean Absolute Percentage Error (MPE): 0.6334 
```
