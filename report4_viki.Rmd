---
title: "report4"
author: "Viktoria Zruttova"
date: "2024-04-07"
output: html_document
---

```{r}
library(car)
library(caret)
library(ggplot2)
library(dplyr)
library(GGally)
library(readxl)
library(MLmetrics)
```

```{r}
#load the dataset
mydata <- read.csv('./data/day.csv')

```

```{r}
#convert 'dteday' column to Date format
mydata$dteday <- as.Date(mydata$dteday)

#season
mydata$season <- cut(mydata$season,
                     breaks = c(0.5, 1.5, 2.5, 3.5, 4.5),
                     labels = c("Winter", "Spring", "Summer", "Fall"))
mydata$season <- factor(mydata$season, levels = c("Winter", "Spring", "Summer", "Fall"))

#workingday
mydata$workingday <- ifelse(mydata$workingday == 0, "Not_Workingday", "Workingday")
mydata$workingday <- factor(mydata$workingday, levels = c("Not_Workingday", "Workingday"))

#weather
mydata$weathersit <- cut(mydata$weathersit,
                         breaks = c(0.5, 1.5, 2.5, 3.5, 4.5),
                         labels = c("Weather_1", "Weather_2", "Weather_3", "Weather_4"))
mydata$weathersit <- factor(mydata$weathersit, levels = c("Weather_1", "Weather_2", "Weather_3", "Weather_4"))

head(mydata)
```

# Data Splitting (Training & Validation): We split the data into training (year = 2011) and validation sets (year = 2012)

The year 2012 dataset will be used for prediction.

```{r}
# Set a seed for reproducibility
# Setting a seed is crucial for reproducibility and consistency of results.
# It ensures that when you run the same code multiple times, you'll ...
# ... obtain the same results.
set.seed(20231103)
# Filter dataset for year 2011 (0) and year 2012 (1)
year2011 <- mydata[mydata$yr == 0, ]
year2012 <- mydata[mydata$yr == 1, ]

# Add 'Type' column and assign values
year2011$Type <- "Training"
year2012$Type <- "Validation"

# Combine training and validation data
mydata <- rbind(year2011, year2012)
```

```{r}
#creating training data our of mydata (training -> data from 2011)
training_data <- subset(mydata, Type == "Training")
summary(training_data)
```

```{r}
#creating training data our of mydata (validatioin -> data from 2012)
validation_data <- subset(mydata, Type == "Validation")
summary(validation_data)
```

We choose 3 questions of interest:

1.  **How does working day affect the number of registered users?** where Y is the numnber of registered users
2.  **How does season affect the number of casual users?** where Y is the sqrt(casual users)
3.  **Is weather correlated to the number of bike rentals?\*** where Y is the count of bikes rented

# Datasets Comparison: Training vs. Validation

### Boxplots for proposed covariates y

```{r}

#registered
ggplot(mydata, aes(x = Type, y = registered, color = Type)) +
  geom_boxplot(position = position_dodge(width = 0.8)) +
  labs(x = "Type", y = "# Registered Users") +
  scale_fill_manual(name = "Dataset Type", values = c("Training" = "blue", "Validation" = "red")) +
  ggtitle("# Registered Users by Type (Training vs. Validation)") +
  theme_bw()

#sqrt(casual)
ggplot(mydata, aes(x = Type, y = sqrt(casual), color = Type)) +
  geom_boxplot(position = position_dodge(width = 0.8)) +
  labs(x = "Type", y = "Square Root # Casual Users") +
  scale_fill_manual(name = "Dataset Type", values = c("Training" = "blue", "Validation" = "red")) +
  ggtitle("Square Root # Casual Users by Type (Training vs. Validation)") +
  theme_bw()

#cnt
ggplot(mydata, aes(x = Type, y = cnt, color = Type)) +
  geom_boxplot(position = position_dodge(width = 0.8)) +
  labs(x = "Type", y = "# Total Users") +
  scale_fill_manual(name = "Dataset Type", values = c("Training" = "blue", "Validation" = "red")) +
  ggtitle("# Total Users by Type (Training vs. Validation)") +
  theme_bw()

# Function to calculate statistics
calculate_statistics <- function(x) {
  return(c(
    Min = min(x, na.rm = TRUE),
    Q1 = quantile(x, 0.25, na.rm = TRUE),
    Median = median(x, na.rm = TRUE),
    Mean = mean(x, na.rm = TRUE),
    Q3 = quantile(x, 0.75, na.rm = TRUE),
    Max = max(x, na.rm = TRUE)
  ))
}


```

### Histogram for Number of Bikes Rented

The Histogram shows that the distribution of cnt in training dataset shows a bimodal shape which has two peaks, while the validation shows left skewed with only one peak.

```{r}
par(mfrow = c(1, 2))
hist(training_data$cnt, main = "Training Set Histogram", xlab = "# bikes rented", col = "blue")
hist(validation_data$cnt, main = "Validation Set Histogram", xlab = "# bikes rented", col = "red")
par(mfrow = c(1, 1))
```

### Histogram for Number of Registered Users and Sqrt_Casual Users

The histograms for registered users are similarly normally distributed, and both are slightly right skewed. The histogram of sqrt(casual) users for each data set shows similarity.

```{r}
#for registered
par(mfrow = c(1, 2))
hist(training_data$registered, main = "Training Set Histogram", xlab = "registered users", col = "blue")
hist(validation_data$registered, main = "Validation Set Histogram", xlab = "registered users", col = "red")
par(mfrow = c(1, 1))

#for casual
par(mfrow = c(1, 2))
hist(sqrt(training_data$casual), main = "Training Set Histogram", xlab = "sqrt casual users", col = "blue")
hist(sqrt(validation_data$casual), main = "Validation Set Histogram", xlab = "sqrt casual users", col = "red")
par(mfrow = c(1, 1))
```

### Categorical Predictors: [weather, season, workingday] Continuous Predictors: [temperature, feeling temperature, wind speed, humidity]

#### Counts and Proportions For categorical
The categorical variables in the two data sets have similar counts and proportions.
```{r}
#model1 <- lm(registered~as.factor(workingday) + as.factor(yr) + as.factor(weather))
#model2 <- lm(sqrt(casual)~as.factor(season) + as.factor(yr) + as.factor(weather) + as.factor(workingday))
#model3 <- lm(cnt ~  temp + wind + hum + as.factor(yr) + as.factor(workingday))

# WorkingDay
training_counts_workingday <- table(training_data$workingday)
validation_counts_workingday <- table(validation_data$workingday)

training_props_workingday <- prop.table(training_counts_workingday)
validation_props_workingday <- prop.table(validation_counts_workingday)

# Season
training_counts_season <- table(training_data$season)
validation_counts_season <- table(validation_data$season)

training_props_season <- prop.table(training_counts_season)
validation_props_season <- prop.table(validation_counts_season)

# Year
training_counts_yr <- table(training_data$yr)
validation_counts_yr <- table(validation_data$yr)

training_props_yr <- prop.table(training_counts_yr)
validation_props_yr <- prop.table(validation_counts_yr)

# Holiday
training_counts_holiday <- table(training_data$holiday)
validation_counts_holiday <- table(validation_data$holiday)

training_props_holiday <- prop.table(training_counts_holiday)
validation_props_holiday <- prop.table(validation_counts_holiday)

# Weathersit
training_counts_weathersit <- table(training_data$weathersit)
validation_counts_weathersit <- table(validation_data$weathersit)

training_props_weathersit <- prop.table(training_counts_weathersit)
validation_props_weathersit <- prop.table(validation_counts_weathersit)

```

```{r}
# Printing out the tables
# For a better layout, you might want to print these one by one or use a custom layout with a package like gridExtra
#working day
cat("Training Set - Working Day Count and Proportion")
print(training_counts_workingday)
print(training_props_workingday)

cat("\nValidation Set - Working Day Count and Proportion")
print(validation_counts_workingday)
print(validation_props_workingday)
cat("\n")
#season
cat("Training Set - Season Count and Proportion")
print(training_counts_season)
print(training_props_season)

cat("\nValidation Set - Season Count and Proportion")
print(validation_counts_season)
print(validation_props_season)
cat("\n")
#year
cat("Training Set - Year Count and Proportion")
print(training_counts_yr)
print(training_props_yr)

cat("\nValidation Set - Year Count and Proportion")
print(validation_counts_yr)
print(validation_props_yr)
cat("\n")
#holiday
cat("Training Set - Holiday Count and Proportion")
print(training_counts_holiday)
print(training_props_holiday)

cat("\nValidation Set - Holiday Count and Proportion")
print(validation_counts_holiday)
print(validation_props_holiday)
cat("\n")
#weathersit
cat("Training Set - Weathersit Count and Proportion")
print(training_counts_weathersit)
print(training_props_weathersit)

cat("\nValidation Set - Weathersit Count and Proportion")
print(validation_counts_weathersit)
print(validation_props_weathersit)
```

#### Histograms For continuous Predictors
All four continuous variables are similarly distributed in the two data sets. In general, the temperature in 2012 is higher than in 2011, and the humidity is relatively lower and more widely spread in 2012 than that in 2011. Also, the wind speed is generally higher in 2012.
```{r}
#For temp
par(mfrow = c(1, 2))
hist(training_data$temp, main = "Training Set Histogram", xlab = "temperature", col = "blue")
hist(validation_data$temp, main = "Validation Set Histogram", xlab = "temperature", col = "red")
par(mfrow = c(1, 1))

#For feeling temp
par(mfrow = c(1, 2))
hist(training_data$atemp, main = "Training Set Histogram", xlab = "feeling temperature", col = "blue")
hist(validation_data$atemp, main = "Validation Set Histogram", xlab = "feeling temperature", col = "red")
par(mfrow = c(1, 1))

#For hum
par(mfrow = c(1, 2))
hist(training_data$hum, main = "Training Set Histogram", xlab = "humidity", col = "blue")
hist(validation_data$hum, main = "Validation Set Histogram", xlab = "humidity", col = "red")
par(mfrow = c(1, 1))

#For windspeed
par(mfrow = c(1, 2))
hist(training_data$windspeed, main = "Training Set Histogram", xlab = "windspeed", col = "blue")
hist(validation_data$windspeed, main = "Validation Set Histogram", xlab = "windspeed", col = "red")
par(mfrow = c(1, 1))
```

# Model Fitting & obtain outputs of the model

## Full models for each question

We are fitting full model which will be used to stepwise regression later. We address multicollinearity issue and delete variables that might cause it.


### Viki trail 1
```{r}
#viki trail 1

full_model1 <- lm(registered ~ as.factor(workingday) + as.factor(season)+ holiday + weekday + temp + hum + windspeed + as.factor(weathersit),
                 data = training_data)

full_model2 <- lm( sqrt(casual)~ as.factor(workingday) + as.factor(season) + holiday + weekday + temp + hum + windspeed + as.factor(weathersit),
                 data = training_data)

full_model3 <- lm( cnt ~ casual +as.factor(workingday) + as.factor(season) + holiday + weekday + temp + hum + windspeed + as.factor(weathersit),
                 data = training_data)

summary(full_model1)
summary(full_model2)
summary(full_model3)

#plot added variables
avPlots(full_model1)
avPlots(full_model2)
avPlots(full_model3)
```

# Stepwise Regression

```{r}
stepwise_model1 <- step(full_model1, direction = "both")
stepwise_model2 <- step(full_model2, direction = "both")
stepwise_model3 <- step(full_model3, direction = "both")
```

```{r}
summary(stepwise_model1)
summary(stepwise_model2)
summary(stepwise_model3)
```

#### model 1: R\^2 = 0.8075

#### model 2: R\^2 = 0.8016

#### model 3: R\^2 = 0.8928

```{r}
avPlots(stepwise_model1)
avPlots(stepwise_model2)
avPlots(stepwise_model3)
```

For vif, values close to 1 indicate no multicollinearity, values between 1-3 indicate mild collinearity which is not ideal but can be considered ok since it won't severaly impact the interpretability of the coefficients, values above indicate severe multicollinearity which is considered unreliable.

```{r}
vif(stepwise_model1)
vif(stepwise_model2)
vif(stepwise_model3)
```

# Validation

```{r}
validation_data$predicted_registered <- predict(stepwise_model1, newdata = validation_data)
validation_data$predicted_sqrtcasual <- predict(stepwise_model2, newdata = validation_data)
validation_data$predicted_count <- predict(stepwise_model3, newdata = validation_data)
```

### Note: here in the predicitons I am unsure why the firts one is negative, in thesecond quetsion, I am unsure whether I should square the original value or not (not sure wthere it predicts the squared casaul count or not). In the third question, it produces Nans???

```{r}
observed_values1 <- validation_data$registered
predicted_values1 <- validation_data$predicted_registered
# Calculate different prediction performance metrics
# Functions from the MLmetrics package
# Common regression metrics
# Calculate the Root Mean Squared Error (RMSE), which measures the ...
# ... average magnitude of prediction errors.
# Lower is better.
rmse1 <- RMSE(predicted_values1, observed_values1)
# same as
# sqrt(mean((observed_values - predicted_values)^2))

# Compute the Mean Absolute Error (MAE), indicating the average absolute ...
# ... difference between predicted and observed values.
# Lower is better.
mae1 <- MAE(predicted_values1, observed_values1)
# same as
# mean(abs(observed_values - predicted_values))

# Calculate the Mean Absolute Percentage Error (MAPE), measuring the ...
# ... average percentage difference between predicted and observed values.
mape1 <- MAPE(predicted_values1, observed_values1)
# same as
# mean(abs(predicted_values-observed_values)/observed_values)

# Determine the R-squared (R²) Score, representing the proportion of the ...
# ... variance in the observed values (of validation data set) ... 
# ... explained by the predicted values from the model.
# Higher is better.
r_squared1 <- R2_Score(predicted_values1, observed_values1)
# same as
# summary(lm(observed_values ~ predicted_values))$r.squared

# Display the calculated metrics
cat("Root Mean Squared Error (RMSE):", round(rmse1, digits = 4), "\n")
cat("Mean Absolute Error (MAE):", round(mae1, digits = 4), "\n")
cat("R-squared (R^2) Score:", round(r_squared1, digits = 4), "\n")
cat("Mean Absolute Percentage Error (MPE):", round(mape1, digits = 4), "\n")



#second question
observed_values2 <- sqrt(validation_data$casual) #should I transform it here??
predicted_values2 <- validation_data$predicted_sqrtcasual
rmse2 <- RMSE(predicted_values2, observed_values2)
mae2 <- MAE(predicted_values2, observed_values2)
mape2 <- MAPE(predicted_values2, observed_values2)
r_squared2 <- R2_Score(predicted_values2, observed_values2)
cat("Root Mean Squared Error (RMSE):", round(rmse2, digits = 4), "\n")
cat("Mean Absolute Error (MAE):", round(mae2, digits = 4), "\n")
cat("R-squared (R^2) Score:", round(r_squared2, digits = 4), "\n")
cat("Mean Absolute Percentage Error (MPE):", round(mape2, digits = 4), "\n")



#third question
observed_values3 <- validation_data$count
predicted_values3 <- validation_data$predicted_count
rmse3 <- RMSE(predicted_values3, observed_values3)
mae3 <- MAE(predicted_values3, observed_values3)
mape3 <- MAPE(predicted_values3, observed_values3)
r_squared3 <- R2_Score(predicted_values3, observed_values3)
cat("Root Mean Squared Error (RMSE):", round(rmse3, digits = 4), "\n")
cat("Mean Absolute Error (MAE):", round(mae3, digits = 4), "\n")
cat("R-squared (R^2) Score:", round(r_squared3, digits = 4), "\n")
cat("Mean Absolute Percentage Error (MPE):", round(mape3, digits = 4), "\n")


```

#### not working for model 3??

## Diagnostics of Training model

we **choose** model 1 because it has best RMSE and R\^2 (it is done for whole 2011 year)

```{r}
#done on model 1
m.mlr <- lm(registered ~ as.factor(workingday) + as.factor(season) + holiday + 
    temp + hum + windspeed + as.factor(weathersit),
                 data = year2011)

```

```{r}
diagnostics_df <- data.frame(
  Residuals = resid(m.mlr),
  Fitted_Values = fitted(m.mlr),
  Standardized_Residuals = rstandard(m.mlr),
  Leverage = hatvalues(m.mlr)
  #Date = year2011$dteday
)

# Create the standardized residuals vs. fitted values plot
ggplot(diagnostics_df, aes(x = Fitted_Values, y = Residuals)) +
  geom_point(col="blue", alpha=0.75) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs( title = "Residuals vs. Fitted Values",
        x = "Fitted Values", y = "Residuals") +
  theme_bw()

# Create the QQ plot
ggplot(diagnostics_df, aes(sample = Standardized_Residuals)) +
  stat_qq(aes(sample = Standardized_Residuals), distribution = qnorm,
          size = 2, col="blue", alpha = 0.75) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Standardized Residual QQ Plot",
       x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_bw()

# Create the sqrt(|standardized residuals|) vs. fitted values plot
ggplot(diagnostics_df, aes(x = Fitted_Values, y = sqrt(abs(Standardized_Residuals)))) +
  geom_point(col="blue", alpha=0.75) +
  labs( title = "Residuals vs. Fitted Values",
        x = "Fitted Values", y = "sqrt(|Standardized Residuals|)") +
  theme_bw()

# Leverage vs Standardized Residuals
ggplot(diagnostics_df, aes(x = Leverage, y = Standardized_Residuals)) +
  geom_point(alpha = 0.75) +
  labs(title = "Standardized Residuals vs. Leverage Plot",
       x = "Leverage", y = "Standardized Residuals") +
  theme_bw()

```

# Prediction

```{r}
validation2012<- subset(validation_data)
validation2012$predicted_registered <- predict(m.mlr, newdata = validation2012)
summary(validation2012$predicted_registered)

```

```{r}
# Extract observed and predicted values
observed_values <- validation2012$registered
predicted_values <- validation2012$predicted_registered
rmse <- RMSE(predicted_values, observed_values)
mae <- MAE(predicted_values, observed_values)
mape <- MAPE(predicted_values, observed_values)
r_squared <- R2_Score(predicted_values, observed_values)


cat("Root Mean Squared Error (RMSE):", round(rmse, digits = 4), "\n")
cat("Mean Absolute Error (MAE):", round(mae, digits = 4), "\n")
cat("R-squared (R^2) Score:", round(r_squared, digits = 4), "\n")
cat("Mean Absolute Percentage Error (MPE):", round(mape, digits = 4), "\n")
```

```{r}
ggplot(validation2012, aes(x = observed_values, y = predicted_values)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(x = "Observed Values", y = "Predicted Values",
       title = "Observed vs. Predicted Values") +
  theme_bw()


# Residuals plot
ggplot(validation2012, aes(x = 1:nrow(validation2012), y = observed_values-predicted_values)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 0, color = "red", linetype = "dashed") +
  labs(x = "Observation Index", y = "Residuals",
       title = "Observed vs. Predicted Values") +
  theme_bw()


```


### Kaylee Trial 1

```{r}
#kaylee trial 1

full_model1 <- lm(registered ~ as.factor(workingday) + as.factor(season)+ as.factor(holiday) + as.factor(weekday) + temp + hum + windspeed + as.factor(weathersit),
                 data = training_data)

full_model2 <- lm( sqrt(casual)~ as.factor(workingday) + as.factor(season)+ as.factor(holiday) + as.factor(weekday) + temp + hum + windspeed + as.factor(weathersit),
                 data = training_data)

full_model3 <- lm( cnt ~ casual+as.factor(workingday) + as.factor(season)+ as.factor(holiday) + as.factor(weekday) + temp + hum + windspeed + as.factor(weathersit),
                 data = training_data)

summary(full_model1)
summary(full_model2)
summary(full_model3)

#plot added variables
avPlots(full_model1)
avPlots(full_model2)
avPlots(full_model3)
```

Compared with Viki's first trial, the AIC for model2 improved as well.(1151 to 1147)
```{r}
stepwise_model1 <- step(full_model1, direction = "both")
stepwise_model2 <- step(full_model2, direction = "both")
stepwise_model3 <- step(full_model3, direction = "both")
```

```{r}
summary(stepwise_model1)
summary(stepwise_model2)
summary(stepwise_model3)
```

```{r}
avPlots(stepwise_model1)
avPlots(stepwise_model2)
avPlots(stepwise_model3)
```

```{r}
vif(stepwise_model1)
vif(stepwise_model2)
vif(stepwise_model3)
```
The above vif is the result after doing following changes: Because of high multicollinearity of workingday and weekday in model2, we will remove one of them from model2. We also remove sqrt from sqrt(casual) in model 3, which changes back to casual, the multicollinearity reduces because GVIF of casual goes down from 4.88 to 3.57, but the R-squared for model3 reduces from 90% to 88%, which is good because effect of overfitting reduces.

### Kaylee Trial 2
```{r}
#kaylee trial 2

full_model1 <- lm(registered ~ as.factor(workingday) + as.factor(season)+ as.factor(holiday) + as.factor(weekday) + temp + hum + windspeed + as.factor(weathersit),
                 data = training_data)

full_model2 <- lm( sqrt(casual)~ as.factor(workingday) + as.factor(season)+ temp + hum + windspeed + as.factor(weathersit),
                 data = training_data)

full_model3 <- lm( cnt ~ casual+as.factor(workingday) + as.factor(season)+ as.factor(holiday) + temp + hum + windspeed + as.factor(weathersit),
                 data = training_data)

summary(full_model1)
summary(full_model2)
summary(full_model3)

#plot added variables
avPlots(full_model1)
avPlots(full_model2)
avPlots(full_model3)
```

```{r}
#kaylee
stepwise_model1 <- step(full_model1, direction = "both")
stepwise_model2 <- step(full_model2, direction = "both")
stepwise_model3 <- step(full_model3, direction = "both")
```

```{r}
summary(stepwise_model1)
summary(stepwise_model2)
summary(stepwise_model3)
```

```{r}
avPlots(stepwise_model1)
avPlots(stepwise_model2)
avPlots(stepwise_model3)
```

```{r}
vif(stepwise_model1)
vif(stepwise_model2)
vif(stepwise_model3)
```


# Validation

```{r}
validation_data$predicted_registered <- predict(stepwise_model1, newdata = validation_data)
validation_data$predicted_sqrtcasual <- predict(stepwise_model2, newdata = validation_data)
validation_data$predicted_count <- predict(stepwise_model3, newdata = validation_data)
```

### Note: In the third question, it produces Nans???

```{r}
observed_values1 <- validation_data$registered
predicted_values1 <- validation_data$predicted_registered
# Calculate different prediction performance metrics
# Functions from the MLmetrics package
# Common regression metrics
# Calculate the Root Mean Squared Error (RMSE), which measures the ...
# ... average magnitude of prediction errors.
# Lower is better.
rmse1 <- RMSE(predicted_values1, observed_values1)
# same as
# sqrt(mean((observed_values - predicted_values)^2))

# Compute the Mean Absolute Error (MAE), indicating the average absolute ...
# ... difference between predicted and observed values.
# Lower is better.
mae1 <- MAE(predicted_values1, observed_values1)
# same as
# mean(abs(observed_values - predicted_values))

# Calculate the Mean Absolute Percentage Error (MAPE), measuring the ...
# ... average percentage difference between predicted and observed values.
mape1 <- MAPE(predicted_values1, observed_values1)
# same as
# mean(abs(predicted_values-observed_values)/observed_values)

# Determine the R-squared (R²) Score, representing the proportion of the ...
# ... variance in the observed values (of validation data set) ... 
# ... explained by the predicted values from the model.
# Higher is better.
r_squared1 <- R2_Score(predicted_values1, observed_values1)
# same as
# summary(lm(observed_values ~ predicted_values))$r.squared

# Display the calculated metrics
cat("Root Mean Squared Error (RMSE):", round(rmse1, digits = 4), "\n")
cat("Mean Absolute Error (MAE):", round(mae1, digits = 4), "\n")
cat("R-squared (R^2) Score:", round(r_squared1, digits = 4), "\n")
cat("Mean Absolute Percentage Error (MPE):", round(mape1, digits = 4), "\n")



#second question
observed_values2 <- sqrt(validation_data$casual) #viki: should I transform it here?? kaylee: I think so
predicted_values2 <- validation_data$predicted_sqrtcasual
rmse2 <- RMSE(predicted_values2, observed_values2)
mae2 <- MAE(predicted_values2, observed_values2)
mape2 <- MAPE(predicted_values2, observed_values2)
r_squared2 <- R2_Score(predicted_values2, observed_values2)
cat("Root Mean Squared Error (RMSE):", round(rmse2, digits = 4), "\n")
cat("Mean Absolute Error (MAE):", round(mae2, digits = 4), "\n")
cat("R-squared (R^2) Score:", round(r_squared2, digits = 4), "\n")
cat("Mean Absolute Percentage Error (MPE):", round(mape2, digits = 4), "\n")



#third question
observed_values3 <- validation_data$count
predicted_values3 <- validation_data$predicted_count
rmse3 <- RMSE(predicted_values3, observed_values3)
mae3 <- MAE(predicted_values3, observed_values3)
mape3 <- MAPE(predicted_values3, observed_values3)
r_squared3 <- R2_Score(predicted_values3, observed_values3)
cat("Root Mean Squared Error (RMSE):", round(rmse3, digits = 4), "\n")
cat("Mean Absolute Error (MAE):", round(mae3, digits = 4), "\n")
cat("R-squared (R^2) Score:", round(r_squared3, digits = 4), "\n")
cat("Mean Absolute Percentage Error (MPE):", round(mape3, digits = 4), "\n")


```
#### model1 R^2 = 80%, R^2 score = -0.84, RMSE = 1931
#### model2 R^2 = 79%, R^2 score = 0.5814, RMSE = 7.64
#### model3 R^3 = 89%, R^2 score = NaN, RMSE = NaN

#### not working for model 3??

## Diagnostics of Training model

we **choose** model 2 because it has best RMSE and R\^2 (it is done for whole 2011 year)

```{r}
#done on model 1
#m.mlr <- lm(registered ~ as.factor(workingday) + as.factor(season) + holiday + 
 #   temp + hum + windspeed + as.factor(weathersit),
  #               data = year2011)
m.mlr <- lm( sqrt(casual)~ as.factor(workingday) + as.factor(season)+ temp + hum + windspeed + as.factor(weathersit),
                 data = year2011)

```

```{r}
diagnostics_df <- data.frame(
  Residuals = resid(m.mlr),
  Fitted_Values = fitted(m.mlr),
  Standardized_Residuals = rstandard(m.mlr),
  Leverage = hatvalues(m.mlr)
  #Date = year2011$dteday
)

# Create the standardized residuals vs. fitted values plot
ggplot(diagnostics_df, aes(x = Fitted_Values, y = Residuals)) +
  geom_point(col="blue", alpha=0.75) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs( title = "Residuals vs. Fitted Values",
        x = "Fitted Values", y = "Residuals") +
  theme_bw()

# Create the QQ plot
ggplot(diagnostics_df, aes(sample = Standardized_Residuals)) +
  stat_qq(aes(sample = Standardized_Residuals), distribution = qnorm,
          size = 2, col="blue", alpha = 0.75) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Standardized Residual QQ Plot",
       x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_bw()

# Create the sqrt(|standardized residuals|) vs. fitted values plot
ggplot(diagnostics_df, aes(x = Fitted_Values, y = sqrt(abs(Standardized_Residuals)))) +
  geom_point(col="blue", alpha=0.75) +
  labs( title = "Residuals vs. Fitted Values",
        x = "Fitted Values", y = "sqrt(|Standardized Residuals|)") +
  theme_bw()

# Leverage vs Standardized Residuals
ggplot(diagnostics_df, aes(x = Leverage, y = Standardized_Residuals)) +
  geom_point(alpha = 0.75) +
  labs(title = "Standardized Residuals vs. Leverage Plot",
       x = "Leverage", y = "Standardized Residuals") +
  theme_bw()

```

# Prediction

```{r}
validation2012<- subset(validation_data)
validation2012$predicted_casual <- predict(m.mlr, newdata = validation2012)
summary(validation2012$predicted_casual)

```

```{r}
# Extract observed and predicted values
observed_values <- sqrt(validation2012$casual)
predicted_values <- validation2012$predicted_casual
rmse <- RMSE(predicted_values, observed_values)
mae <- MAE(predicted_values, observed_values)
mape <- MAPE(predicted_values, observed_values)
r_squared <- R2_Score(predicted_values, observed_values)


cat("Root Mean Squared Error (RMSE):", round(rmse, digits = 4), "\n")
cat("Mean Absolute Error (MAE):", round(mae, digits = 4), "\n")
cat("R-squared (R^2) Score:", round(r_squared, digits = 4), "\n")
cat("Mean Absolute Percentage Error (MPE):", round(mape, digits = 4), "\n")
```

```{r}
ggplot(validation2012, aes(x = observed_values, y = predicted_values)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(x = "Observed Values", y = "Predicted Values",
       title = "Observed vs. Predicted Values") +
  theme_bw()


# Residuals plot
ggplot(validation2012, aes(x = 1:nrow(validation2012), y = observed_values-predicted_values)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 0, color = "red", linetype = "dashed") +
  labs(x = "Observation Index", y = "Residuals",
       title = "Observed vs. Predicted Values") +
  theme_bw()


```
